{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Subtitles creation with `whisperx`\n",
        "\n",
        "In this notebook, we'll undertake a series of operations focused on the management and processing of audio files using the `whisperx` tool. Through the following steps, we'll address the setup of our environment, clean up any pre-existing files, extract data from audio transcriptions, and then convert them into a specific subtitle format.\n",
        "\n",
        "The `whisperx` tool allows us to process audio files to obtain detailed transcriptions. This notebook serves as a hands-on guide to execute this process from start to finish within the Google Colab environment.\n"
      ],
      "metadata": {
        "id": "pPS1bRAT-wyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following commands are used to install specific libraries directly from their GitHub repositories:\n",
        "\n",
        "- The first command installs the `whisperx` library from the repository maintained by `m-bain`.\n",
        "- The second command installs the `whisperX_subs` library from the repository maintained by `lrubiorod`.\n",
        "\n",
        "**Important Reminder**: After installing these libraries, you might encounter a warning indicating that certain packages were previously imported in the runtime. To ensure that you are using the newly installed versions of the packages, it's recommended to **restart the Colab runtime**. Look out for messages like:\n",
        "\n",
        "***WARNING: The following packages were previously imported in this runtime:\n",
        "[pydevd_plugins] You must restart the runtime in order to use newly installed versions.***\n",
        "\n",
        "To restart the runtime, you can click on the \"Runtime\" menu at the top and select \"Restart runtime\"."
      ],
      "metadata": {
        "id": "8cccUNw343-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmJgKU5hCdoe"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/lrubiorod/whisperX_subs\n",
        "!pip install git+https://github.com/m-bain/whisperx.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, to access files from Google Drive directly within the Colab environment, it's necessary to mount the Google Drive. The following lines of code are used for this purpose:"
      ],
      "metadata": {
        "id": "L1Ir_vKq55Pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hUapDJjgClRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code below, we are setting up the path to a specific audio file located in the user's Google Drive.\n",
        "\n",
        "- `directory_path`: This is the path to the directory where the audio file is located.\n",
        "- `file_name`: The name of the audio file without its extension.\n",
        "- `ext`: The extension of the audio file, in this case, `.wav`.\n",
        "- `path_to_input`: This constructs the full path to the audio file by concatenating the directory, file name, and extension.\n",
        "- `max_char`: A predefined limit, possibly representing the maximum number of characters to be processed or displayed from the audio's transcription.\n",
        "\n",
        "Let's set these variables:"
      ],
      "metadata": {
        "id": "QGcfKMCr6b3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the directory containing the audio file in Google Drive\n",
        "directory_path = \"/content/drive/My Drive/[DIRECTORY PATH]/\"\n",
        "\n",
        "# Name of the audio file (without extension)\n",
        "file_name = \"[FILE NAME]\"\n",
        "\n",
        "# Audio file extension\n",
        "ext = \".wav\"\n",
        "\n",
        "# Construct the full path to the audio file\n",
        "path_to_input = directory_path + file_name + ext\n",
        "\n",
        "# Set the maximum number of characters (e.g., for transcription display or processing)\n",
        "max_char = 50\n",
        "\n",
        "# Specify the language code for processing.\n",
        "# For a list of available language codes, see: https://github.com/m-bain/whisperX/blob/main/whisperx/utils.py\n",
        "source_language = \"en\"\n"
      ],
      "metadata": {
        "id": "aQEE28vYDfI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below provides two different commands for processing audio using the `whisperx` tool.\n",
        "\n",
        "- **Basic Command**: The first command processes the audio file using the default settings.\n",
        "- **Advanced Command**: The second command (currently commented out) uses advanced models for alignment. To use this advanced command, you need to:\n",
        "  1. Comment out the first command.\n",
        "  2. Uncomment the second command.\n",
        "  3. Provide a Hugging Face access token by replacing `[HUGGING FACE TOKEN]`.\n",
        "\n",
        "To generate a Hugging Face token, visit [this link](https://huggingface.co/settings/tokens). Ensure that you've accepted the user agreement for the following models to enable Speaker Diarization:\n",
        "  - [Segmentation](https://huggingface.co/pyannote/segmentation)\n",
        "  - [Voice Activity Detection (VAD)](https://huggingface.co/pyannote/voice-activity-detection)\n",
        "  - [Speaker Diarization](https://huggingface.co/pyannote/speaker-diarization)\n",
        "\n",
        "Let's set up the commands:"
      ],
      "metadata": {
        "id": "hXqzVxLh7po3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic audio processing command\n",
        "!whisperx \"{path_to_input}\" \\\n",
        "--model large-v2 \\\n",
        "--language \"{source_language}\" \\\n",
        "--output_format json \\\n",
        "--output_dir \"{directory_path}\"\n",
        "\n",
        "# If you wish to use advanced models for alignment, comment the above command and uncomment below:\n",
        "# Ensure to provide your Hugging Face token after accepting the user agreements for the necessary models\n",
        "#hf_token = \"[HUGGING FACE TOKEN]\"\n",
        "#!whisperx \"{path_to_input}\" \\\n",
        "#--model large-v2 \\\n",
        "#--language \"{source_language}\" \\\n",
        "#--align_model WAV2VEC2_ASR_LARGE_LV60K_960H \\\n",
        "#--batch_size 32 \\\n",
        "#--hf_token $hf_token \\\n",
        "#--output_format json \\\n",
        "#--output_dir \"{directory_path}\"\n"
      ],
      "metadata": {
        "id": "wX_hhaK4Df5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code accomplishes three main tasks:\n",
        "\n",
        "1. **Data Extraction**: Retrieves the transcription data from a JSON file that corresponds to the audio file.\n",
        "2. **Conversion & Translation**: Processes this data to convert it into the SRT format. Additionally, there's a provision for translating the subtitles to a different language (e.g., English to Spanish).\n",
        "3. **Saving**: After conversion (and optional translation), the result is saved as a new `.srt` file.\n",
        "\n",
        "Let's proceed with these operations:\n"
      ],
      "metadata": {
        "id": "QufdqtFk8PlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required module from the `whisperx_subs` package\n",
        "import whisperx_subs.whisperx_subs as ws\n",
        "\n",
        "# Load the content from the associated JSON file\n",
        "json_path = directory_path + file_name + \".json\"\n",
        "with open(json_path, \"r\") as file:\n",
        "    json_content = file.read()\n",
        "\n",
        "# Specify no languages to translate\n",
        "langs_to_translate = []\n",
        "# Uncomment the lines below and specify the languages to translate that you wish to use.\n",
        "#langs_to_translate = ['es','fr']\n",
        "\n",
        "# Convert the JSON content into SRT format\n",
        "srt_results = ws.convert_json_to_srt(json_content, max_char, langs_to_translate)\n",
        "\n",
        "# Save the converted SRT content to new .srt files\n",
        "for result, lang in zip(srt_results, [source_language] + langs_to_translate):\n",
        "    suffix = f\"({lang})\" if lang != source_language else \"\"\n",
        "    srt_path = directory_path + file_name + suffix + \".srt\"\n",
        "    with open(srt_path, \"w\") as file:\n",
        "        file.write(result)\n",
        "\n",
        "print('\\nProcess completed!')\n"
      ],
      "metadata": {
        "id": "-kJOq_TxxIT2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}